"""
Trvi.io job used to extract features from the common craw corpus
"""

common_crawl_url =   's3://bogus-key:bogus-secrect@aws-publicdatasets/common-crawl/parse-output/segment/'

cc_features = rule(common_crawl_url,'cc_features')

@cc_features.map_reader
def beautiful_soup(stream, size, url, params):
  from bs4 import BeautifulSoup
  for doc in stream:
    if doc['content-type'] == 'text/html':
      doc['html'] = BeautifulSoup(doc['payload'])
    yield doc

@cc_features.map
def map(doc, params):
  from features import extractor
  return extractor(doc)



