"""
Trvi.io job used to extract features from the common craw corpus
"""

#common_crawl_url =   's3://AKIAJ6PF5PWHFME4GA5A:hoHLkSwHztDHFEJr0z3iMrvQndgLfrDraxK3Oru7@aws-publicdatasets/common-crawl/parse-output/segment/'

common_crawl_url = 's3://AKIAJ6PF5PWHFME4GA5A:hoHLkSwHztDHFEJr0z3iMrvQndgLfrDraxK3Oru7@aws-publicdatasets/common-crawl/crawl-002/'
cc_features = rule(common_crawl_url,'cc_features')
cc_features.param('maxinput', 1)

@cc_features.map_reader
def beautiful_soup(stream, size, url, params):

  from bs4 import BeautifulSoup
  for doc in stream:
    if doc['content_type'] == 'text/html':
      try:
        doc['payload'] = doc['payload'].decode('utf-8','replace')
        doc['html'] = BeautifulSoup(doc['payload'])
      except Exception, e:
        print "Error parsing: {}".format(doc['url'])
  yield doc

@cc_features.map
def map(doc, params):
  #from features import extractor
  #return extractor(doc)
  for key in doc.keys():
    yield key, 1

@cc_features.reduce
def reduce(iter, params):
  return  ( (key,1) for key in set((k for k,v in iter)))

  #return iter
