"""
Trvi.io job used to extract features from the common craw corpus
"""

#common_crawl_url =   's3://AKIAJ6PF5PWHFME4GA5A:hoHLkSwHztDHFEJr0z3iMrvQndgLfrDraxK3Oru7@aws-publicdatasets/common-crawl/parse-output/segment/'

common_crawl_url = 's3://AKIAJ6PF5PWHFME4GA5A:hoHLkSwHztDHFEJr0z3iMrvQndgLfrDraxK3Oru7@aws-publicdatasets/common-crawl/crawl-002/'
cc_features = rule(common_crawl_url,'cc_features')
cc_features.param('maxinput', 1)

@cc_features.map_reader
def beautiful_soup(stream, size, url, params):
  print "soups on"
  from bs4 import BeautifulSoup, SoupStrainer

  def links_and_scripts(name, attrs):
    return name in ('a', 'script')

  parse_only = SoupStrainer(links_and_scripts)
  for doc in stream:
    if doc['content_type'] == 'text/html':
      try:
        doc['payload'] = doc['payload'].decode('utf-8','replace')
        doc['html'] = BeautifulSoup(doc['payload'], parseOnlyThese=parse_only)
      except Exception, e:
        doc['parse_exception'] = e
    yield doc

@cc_features.map
def map(doc, params):
  yield 'doc', 1
  #from features import extractor
  #return extractor(doc, params)

@cc_features.reduce
def reduce(iter, params):
  return iter
